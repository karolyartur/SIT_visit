{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">2.1 Artificial Neuron Structure</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "In this lesson we will learn how the basic unit of a neural network, a single neuron works.\n",
    "<br>\n",
    "First we will discuss the theory behind the artificial neuron and then we code examples\n",
    "<br>\n",
    "with the help of the NumPy module.\n",
    "\n",
    "First of all, let's import the NumPy module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atrificial neuron</h2>\n",
    "\n",
    "The functioning of an artificial neuron can be best understood through its analogy with the biological neuron.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*v4f4-nMoRMNrtUZG.png\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "Biological neurons take impulses from other neurons through their dendrites. If the intensity of these impulses\n",
    "<br>\n",
    "exceed a given level the neuron also emits a signal on its axon. The impulses from other neurons can either be\n",
    "<br>\n",
    "inhibited or prohibited, so the neuron is able to consider different inputs with different significance level to\n",
    "<br>\n",
    "its own output. In the artificial neuron model, the inputs of the model $(x_i)$ represent the intensity of impulses\n",
    "<br>\n",
    "coming from the other neurons. The inhibition/prohibition is achieved by assigning weights $(w_i)$ to the inputs.\n",
    "<br>\n",
    "In the cell body the accumulation of the input inpulses is carried out, that is modelled with a summation in the\n",
    "<br>\n",
    "artificial neuron model. The property that the biological neuron only activates when the intensity of the inputs\n",
    "<br>\n",
    "is greater than a certain level is achieved with a nonlinear function, called activation function in the artificial\n",
    "<br>\n",
    "nauron. The activation function serves as a threshold on the output. The final output of the neuron can be\n",
    "<br>\n",
    "fed to other neurons through the axon terminals.\n",
    "\n",
    "So an artificial neuron does the following:\n",
    "\n",
    " - Take an input vector $\\mathbf{x}$ and a weight vector $\\mathbf{w}$\n",
    " - Compute the sum of the weighted inputs $net$ as the dot product of the vectors $\\mathbf{x}$ and $\\mathbf{w}$:\n",
    " \n",
    " $$net = \\sum_{i=0}^n w_ix_i$$\n",
    " \n",
    " - Apply the activation function $f()$ on $net$ to compute the final result $y$:\n",
    " \n",
    " $$y = f(net)$$\n",
    " \n",
    "Here $n$ is the number of inputs. Notice that there is an extra input (indexed with $0$). This input does not count\n",
    "<br>\n",
    "in the number of inputs $(n)$ and its value is always $1$. The associated weight $w_0$ is often referred to as bias\n",
    "<br>\n",
    "and it is signed with $b$ instead of $w_0$.\n",
    "\n",
    "So in a compact form the output of a single artificial neuron can be computed like:\n",
    "\n",
    "$$y=f\\left(b+\\sum_{i=0}^n w_ix_i\\right)$$\n",
    "\n",
    "The only one thing left to discuss is the activation function $f()$.\n",
    "<br>\n",
    "The purpose of the activation function is to threshold the weighted sum of the inputs and in most cases, to squash the\n",
    "<br>\n",
    "output into a region, so the output value will not be too large or too low which would make further computations difficult.\n",
    "\n",
    "The first idea that comes to mind is a simple step function (with a hard threshold) like:\n",
    "\n",
    "$$\n",
    "f(\\mathrm{x})=    \n",
    "\\begin{cases}\n",
    "      1 & \\text{if $\\mathrm{x} >$ threshold}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "But it turns out that other activation functions can be better applied in case of more complex problems.\n",
    "<br>\n",
    "The most popular activation functions are the sigmoid, the hyperbolic tangent, the ReLU, Leaky ReLU and softmax\n",
    "<br>\n",
    "activations. These can be computed like:\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$$\\sigma (\\mathrm{x})=\\frac{1}{1+e^{-\\mathrm{x}}}$$\n",
    "\n",
    "Hyperbolic tangent:\n",
    "\n",
    "$$f_{tanh}(\\mathrm{x})=\\tanh{(\\mathrm{x})}=\\frac{\\sinh{(\\mathrm{x})}}{\\cosh{(\\mathrm{x})}}$$\n",
    "\n",
    "ReLU:\n",
    "\n",
    "$$R(\\mathrm{x}) = \\max{(0,\\mathrm{x})}$$\n",
    "\n",
    "\n",
    "Leaky ReLU:\n",
    "\n",
    "$$LR(\\mathrm{x}) = \\max{(\\alpha \\mathrm{x},\\mathrm{x})}, \\text{ where $\\alpha = 0.01$}$$\n",
    "\n",
    "The Softmax function is an odd one between these. We will discuss it a bit later. First we have to see how these\n",
    "<br>\n",
    "activation functions look like and what are they good for.\n",
    "\n",
    "Let's plot the the above mentionsed activation functions:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
