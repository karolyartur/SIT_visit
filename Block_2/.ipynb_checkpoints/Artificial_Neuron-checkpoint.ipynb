{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">2.1 Artificial Neuron Structure</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "In this lesson we will learn how the basic unit of a neural network, a single neuron works.\n",
    "<br>\n",
    "First we will discuss the theory behind the artificial neuron and then we code examples\n",
    "<br>\n",
    "with the help of the NumPy module.\n",
    "\n",
    "First of all, let's import the NumPy module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atrificial neuron</h2>\n",
    "\n",
    "The functioning of an artificial neuron can be best understood through its analogy with the biological neuron.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*v4f4-nMoRMNrtUZG.png\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "Biological neurons take impulses from other neurons through their dendrites. If the intensity of these impulses\n",
    "<br>\n",
    "exceed a given level the neuron also emits a signal on its axon. The impulses from other neurons can either be\n",
    "<br>\n",
    "inhibited or prohibited, so the neuron is able to consider different inputs with different significance level to\n",
    "<br>\n",
    "its own output. In the artificial neuron model, the inputs of the model $(x_i)$ represent the intensity of impulses\n",
    "<br>\n",
    "coming from the other neurons. The inhibition/prohibition is achieved by assigning weights $(w_i)$ to the inputs.\n",
    "<br>\n",
    "In the cell body the accumulation of the input inpulses is carried out, that is modelled with a summation in the\n",
    "<br>\n",
    "artificial neuron model. The property that the biological neuron only activates when the intensity of the inputs\n",
    "<br>\n",
    "is greater than a certain level is achieved with a nonlinear function, called activation function in the artificial\n",
    "<br>\n",
    "nauron. The activation function serves as a threshold on the output. The final output of the neuron can be\n",
    "<br>\n",
    "fed to other neurons through the axon terminals.\n",
    "\n",
    "So an artificial neuron does the following:\n",
    "\n",
    " - Take an input vector $\\mathbf{x}$ and a weight vector $\\mathbf{w}$\n",
    " - Compute the sum of the weighted inputs $net$ as the dot product of the vectors $\\mathbf{x}$ and $\\mathbf{w}$:\n",
    " \n",
    " $$net = \\sum_{i=0}^n w_ix_i$$\n",
    " \n",
    " - Apply the activation function $f()$ on $net$ to compute the final result $y$:\n",
    " \n",
    " $$y = f(net)$$\n",
    " \n",
    "Here $n$ is the number of inputs. Notice that there is an extra input (indexed with $0$). This input does not count\n",
    "<br>\n",
    "in the number of inputs $(n)$ and its value is always $1$. The associated weight $w_0$ is often referred to as bias\n",
    "<br>\n",
    "and it is signed with $b$ instead of $w_0$.\n",
    "\n",
    "So in a compact form the output of a single artificial neuron can be computed like:\n",
    "\n",
    "$$y=f\\left(b+\\sum_{i=0}^n w_ix_i\\right)$$\n",
    "\n",
    "The only one thing left to discuss is the activation function $f()$.\n",
    "<br>\n",
    "The purpose of the activation function is to threshold the weighted sum of the inputs and in most cases, to squash the\n",
    "<br>\n",
    "output into a region, so the output value will not be too large or too low which would make further computations difficult.\n",
    "\n",
    "The first idea that comes to mind is a simple step function (with a hard threshold) like:\n",
    "\n",
    "$$\n",
    "f(\\mathrm{x})=    \n",
    "\\begin{cases}\n",
    "      1 & \\text{if $\\mathrm{x} >$ threshold}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "But it turns out that other activation functions can be better applied in case of more complex problems.\n",
    "<br>\n",
    "The most popular activation functions are the sigmoid, the hyperbolic tangent, the ReLU, Leaky ReLU and softmax\n",
    "<br>\n",
    "activations. These can be computed like:\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$$\\sigma (\\mathrm{x})=\\frac{1}{1+e^{-\\mathrm{x}}}$$\n",
    "\n",
    "Hyperbolic tangent:\n",
    "\n",
    "$$\\tanh{(\\mathrm{x})}=\\frac{\\sinh{(\\mathrm{x})}}{\\cosh{(\\mathrm{x})}}=\\frac{e^\\mathrm{x}-e^{-\\mathrm{x}}}{e^\\mathrm{x}+e^{-\\mathrm{x}}}$$\n",
    "\n",
    "ReLU:\n",
    "\n",
    "$$R(\\mathrm{x}) = \\max{(0,\\mathrm{x})}$$\n",
    "\n",
    "\n",
    "Leaky ReLU:\n",
    "\n",
    "$$LR(\\mathrm{x}) = \\max{(\\alpha \\mathrm{x},\\mathrm{x})}, \\text{ where $\\alpha = 0.01$}$$\n",
    "\n",
    "The Softmax function is an odd one between these. We will discuss it a bit later. First we have to see how these\n",
    "<br>\n",
    "activation functions look like and what are they good for.\n",
    "\n",
    "Let's plot the the above mentioned activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The inputs for the functions\n",
    "x = np.arange(-4.0, 4.0, 0.01)\n",
    "\n",
    "#Activation functions applied to the inputs\n",
    "th = np.copy(x)\n",
    "th[th>0] = 1\n",
    "th[th<0] = 0\n",
    "sigm = 1/ (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "ReLU = np.clip(x, 0, None)\n",
    "LR = np.copy(x)\n",
    "LR[LR<0]=0.01*LR[LR<0]\n",
    "\n",
    "#Plot the outputs of the activation functions\n",
    "plt.figure(1)\n",
    "plt.plot(x, x, label='Linear')\n",
    "plt.plot(x, th, label='Step')\n",
    "plt.plot(x, sigm, label='Sigmoid')\n",
    "plt.plot(x, tanh, label='Hyperbolic tangent')\n",
    "plt.title('Activation functions1', fontsize=20)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('f(x)', fontsize=10)\n",
    "plt.legend()\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-1.2,1.2)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(x, ReLU, label='ReLU')\n",
    "plt.plot(x, LR, label='Leaky ReLU')\n",
    "plt.title('Activation functions2', fontsize=20)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('f(x)', fontsize=10)\n",
    "plt.legend()\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-0.1,1.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, the properties of the different activation functions can be seen.\n",
    "\n",
    "The linear activation function is used if the output should not be bound and the prediction of\n",
    "<br>\n",
    "continuous values is required, like in regression problems.\n",
    "\n",
    "The sigmoid function will output values between $0$ and $1$ $(0<\\sigma (\\mathrm{x})<1)$.\n",
    "<br>\n",
    "The output of this activation function can be interpreted as a probability, so it is widely used in\n",
    "<br>\n",
    "classification problems, where the output means the probability of belonging to a class.\n",
    "\n",
    "The hyperbolic tangent activation function takes values between $-1$ and $1$ $(-1<\\tanh{(\\mathrm{x})}<1)$.\n",
    "<br>\n",
    "This activation function is videly used in case of binary classification problems, when the output should tell\n",
    "<br>\n",
    "if the sample belongs to either one or the other class.\n",
    "\n",
    "The ReLU activation function solves the saturation problem of the sigmoid and hyperbolic tangent functions.\n",
    "\n",
    "The Leaky ReLU solves the dying gradient problem of the ReLU activation function (The dying gradient is the reason\n",
    "<br>\n",
    "why we don't use the step function for activation.\n",
    "\n",
    "\n",
    "Now it is time to discuss the Softmax function.\n",
    "\n",
    "Imagine that you would like to classify the inputs into tree classes. According the the formerly introduced\n",
    "<br>\n",
    "activation functions you would choose the sigmoid activation and apply three neurons to calculate the outputs\n",
    "<br>\n",
    "where each neuron would represent a single class and their output is the probability that the given sample belongs\n",
    "<br>\n",
    "to the given class. However, you have samples that cannot belong to multiple classes at the same time. So the\n",
    "<br>\n",
    "class labels are mutually exclusive. But if you apply sigmoid activation than there is no garantee that two or three\n",
    "<br>\n",
    "neurons cannot activate for the same sample. This is where the softmax function is used.\n",
    "\n",
    "The input of the softmax function is a **vector** not a scalar and it outputs a **probability distribution** instead of\n",
    "<br>\n",
    "single probabilities. This means that the output has the same dimension as the input vector and its values are between\n",
    "<br>\n",
    "zero and one and if you sum all the values of the output vector it adds up to one.\n",
    "\n",
    "The softmax function can be computed like:\n",
    "\n",
    "$$Softmax(\\mathbf{x})_j = \\frac{e^{\\mathrm{x}_j}}{\\sum_{k=1}^Ke^{\\mathrm{x}_k}}$$\n",
    "\n",
    ", where $j = 1 \\dots K,\\text{ and } \\mathbf{x}=[x_1,\\dots,x_K]$\n",
    "\n",
    "<p style=\"margin-top:2em;\">Now that we know how the output of an artificial neuron can be computed, it is time to implement one in which we specify the weights:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create functions implementing the activation functions\n",
    "def sigmoid(x):\n",
    "    return (1/ (1 + np.exp(-x)))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.tanh(x))\n",
    "\n",
    "def relu(x):\n",
    "    return (np.clip(x, 0, None))\n",
    "\n",
    "#Define a function for calculating the output of a single artificial neuron expecting the inputs and weights\n",
    "def neuron(inputs, weights, activation):\n",
    "    '''Function for defining an artificial neuron\n",
    "    \n",
    "    Inputs:\n",
    "        inputs (numpy.ndarray) - The inputs of the neuron\n",
    "        weights (numpy.ndarray) - The weights of the neuron (The length of the weights array must be len(inputs)+1\n",
    "                                    because the last element of weights is the bias.)\n",
    "        activation (function) - The activation function to use\n",
    "        \n",
    "    Returns:\n",
    "        (?) -The output of the neuron\n",
    "    '''\n",
    "    \n",
    "    inputs = np.append(inputs, 1)\n",
    "    return (activation(weights.dot(inputs)))\n",
    "\n",
    "#Compute the output of some neurons with fixed weights and an arbitrary activations\n",
    "#for four samples: [0,0], [0,1], [1,0] and [1,1]\n",
    "\n",
    "samples = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "#Weights: [2.5, 2.8, -4.2], activation: sigmoid\n",
    "weights = np.array([2.5, 2.8, -4.2])\n",
    "\n",
    "for sample in samples:\n",
    "    out = neuron(sample, weights, sigmoid)\n",
    "    print('Input:', sample, 'output:', out, 'sample is member of this class? (1=true, 0=false)', int(out>0.5))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "#Plot the neuron activations:\n",
    "y, x = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "z=np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        z[(i,j)]=neuron(np.array([i/100,j/100]), weights, sigmoid)\n",
    "    \n",
    "plt.figure(1)\n",
    "z_min, z_max = z.min(), z.max()\n",
    "plt.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "plt.title('First neuron activations')\n",
    "plt.colorbar()\n",
    "    \n",
    "#Weights: [2.5, 2.8, -4.2], activation: sigmoid\n",
    "weights = np.array([2.5, 2.8, -1.8])\n",
    "\n",
    "for sample in samples:\n",
    "    out = neuron(sample, weights, sigmoid)\n",
    "    print('Input:', sample, 'output:', out, 'sample is member of this class? (1=true, 0=false)', int(out>0.5))\n",
    "    \n",
    "#Plot the neuron activations:\n",
    "z=np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        z[(i,j)]=neuron(np.array([i/100,j/100]), weights, sigmoid)\n",
    "    \n",
    "plt.figure(2)\n",
    "z_min, z_max = z.min(), z.max()\n",
    "plt.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "plt.title('Second neuron activations')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with single neurons that have only two inputs we are able to create logical functions such as and, or ...\n",
    "\n",
    "By appying several neurons paralelly on the same data we can compute may outputs at the same time. However, it is\n",
    "<br>\n",
    "important to notice, that the decision boundary (the threshold level) belonging to a single neuron will always be linear.\n",
    "<br>\n",
    "So, nonlinear functions such as the exclusive or (XOr) cannot be represented with a single neuron.\n",
    "<br>\n",
    "Although the XOr function can be computed as the combination of simpler linear functions like the ones we already implemented.\n",
    "<br>\n",
    "This is the idea from where neural networks, and thus deep learning originates.\n",
    "\n",
    "<h2>Excersise 2.1</h2>\n",
    "Based on the previously implemented artificial neurons, create a network of these neurons to compute the XOr connection\n",
    "<br>\n",
    "between two logical variables. For this you need to:\n",
    "\n",
    " - Express the XOr function as a combination of linear logical functions\n",
    " - Implement the missing functions with artificial neurons\n",
    " - Combine the neurons into a network that computes the XOr function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See solution here: [Excersise 2.1 solution](Excersise_2_1.ipynb)\n",
    "\n",
    "Continue: [2.2 Training Process and Gradient Descent](Training_Gradient_Descent.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
