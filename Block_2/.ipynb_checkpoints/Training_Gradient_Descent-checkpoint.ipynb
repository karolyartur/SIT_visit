{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">2.2 Training Process and Gradient Descent</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "We already saw why artificial neurons and neural networks are such powerful computing tools. Now we will se why they\n",
    "<br>\n",
    "are practical. The training process enables us to tune the weights of a neural network model automatically.\n",
    "<br>\n",
    "For this purpose we will use the Gradient Descent algorithm.\n",
    "\n",
    "First of all import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient and Optimization</h2>\n",
    "\n",
    "In order to train a neural netwok, first of all, a goal for the network have to be defined.\n",
    "<br>\n",
    "This goal can appear in several forms. It can be to classify the inputs, perform clustering, regress\n",
    "<br>\n",
    "continuous values, etc. The objective is to modify the weights of the network so it performs\n",
    "<br>\n",
    "the given task well, a.k.a. with minimal error.\n",
    "\n",
    "So in order to train a neural network we should define a function that computes the error\n",
    "<br>\n",
    "of the predictions of the neural network. During training we use iterative optimization methods\n",
    "<br>\n",
    "to minimize the predicion error across the training dataset with respect to the weights of the\n",
    "<br>\n",
    "neural network.\n",
    "\n",
    "The error function will be a multi-variable function. While in case of functions with a single\n",
    "<br>\n",
    "variable the derivative can be used to search for the minima, in case of multi-variable functions\n",
    "<br>\n",
    "a multi-variable generalization of the derivative have to e used. This is the gradient.\n",
    "\n",
    "The gradient of a multi-variable function is a vector with the dimension of the number of variables.\n",
    "<br>\n",
    "The elements of the gradient are the partial derivatives of the function with respect to the\n",
    "<br>\n",
    "correspoding variables. The gradient can be formulated like:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_1}},\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_2}},\n",
    "\\dots,\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_n}}\n",
    "\\right]$$\n",
    "\n",
    ",where $f(\\mathbf{x})$ is the multi-variable function,\n",
    "<br>\n",
    "<br>\n",
    "$\\nabla f(\\mathbf{x})$ is the gradient,\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_i}}$ is the $i^{th}$ partial derivative and $\\mathbf{x} = \\left[\\mathrm{x_1}, \\mathrm{x_2}, \\dots, \\mathrm{x_n}\\right],\\space (i \\in \\{1,2, \\dots, n\\})$\n",
    "\n",
    "Similar to the derivative, the gradient also represents the slope of the function. Furthermore\n",
    "<br>\n",
    "it is a vector that points in the direction of the steepest increase in the function and its\n",
    "<br>\n",
    "magnitude is the steepness of the function in that given direction.\n",
    "\n",
    "Let's see, how the gradient of a simple multi-variable function, $f(x,y) = x^2+y^2$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function:\n",
    "def function(input):\n",
    "    return (input[0]**2+input[1]**2)\n",
    "\n",
    "def gradient(input):\n",
    "    return(np.array([2*input[0],2*input[1]]))\n",
    "\n",
    "#plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = y = np.arange(-3.0, 3.0, 0.05)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zs = np.array([function([x,y]) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z = zs.reshape(X.shape)\n",
    "\n",
    "ax.plot_surface(X, Y, Z)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Function')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
