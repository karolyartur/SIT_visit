{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">2.2 Training Process and Gradient Descent</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "We already saw why artificial neurons and neural networks are such powerful computing tools. Now we will see why they\n",
    "<br>\n",
    "are practical. The training process enables us to tune the weights of a neural network model automatically.\n",
    "<br>\n",
    "For this purpose we will use the Gradient Descent algorithm.\n",
    "\n",
    "First of all import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient and Optimization</h2>\n",
    "\n",
    "In order to train a neural netwok, first of all, a goal for the network have to be defined.\n",
    "<br>\n",
    "This goal can appear in several forms. It can be to classify the inputs, perform clustering, regress\n",
    "<br>\n",
    "continuous values, etc. The objective is to modify the weights of the network so it performs\n",
    "<br>\n",
    "the given task well, a.k.a. with minimal error.\n",
    "\n",
    "So in order to train a neural network we should define a function that computes the error\n",
    "<br>\n",
    "of the predictions of the neural network. During training we use iterative optimization methods\n",
    "<br>\n",
    "to minimize the predicion error across the training dataset with respect to the weights of the\n",
    "<br>\n",
    "neural network.\n",
    "\n",
    "The error function will be a multi-variable function. While in case of functions with a single\n",
    "<br>\n",
    "variable the derivative can be used to search for the minima, in case of multi-variable functions\n",
    "<br>\n",
    "a multi-variable generalization of the derivative have to e used. This is the gradient.\n",
    "\n",
    "The gradient of a multi-variable function is a vector with the dimension of the number of variables.\n",
    "<br>\n",
    "The elements of the gradient are the partial derivatives of the function with respect to the\n",
    "<br>\n",
    "correspoding variables. The gradient can be formulated like:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_1}},\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_2}},\n",
    "\\dots,\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_n}}\n",
    "\\right]$$\n",
    "\n",
    ",where $f(\\mathbf{x})$ is the multi-variable function,\n",
    "<br>\n",
    "<br>\n",
    "$\\nabla f(\\mathbf{x})$ is the gradient,\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathrm{x_i}}$ is the $i^{th}$ partial derivative and $\\mathbf{x} = \\left[\\mathrm{x_1}, \\mathrm{x_2}, \\dots, \\mathrm{x_n}\\right],\\space (i \\in \\{1,2, \\dots, n\\})$\n",
    "\n",
    "Similar to the derivative, the gradient also represents the slope of the function. Furthermore\n",
    "<br>\n",
    "it is a vector that points in the direction of the steepest increase in the function and its\n",
    "<br>\n",
    "magnitude is the steepness of the function in that given direction.\n",
    "\n",
    "Let's see, how the gradient of a simple multi-variable function, $f(x,y) = x^2+y^2$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function:\n",
    "def function(inp):\n",
    "    return (inp[0]**2+inp[1]**2)\n",
    "\n",
    "def gradient(inp):\n",
    "    return(np.array([2*inp[0],2*inp[1]]))\n",
    "\n",
    "#Plot function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = y = np.arange(-3.0, 3.0, 0.05)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zs = np.array([function([x,y]) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z = zs.reshape(X.shape)\n",
    "\n",
    "ax.plot_surface(X, Y, Z)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Function with its gradient')\n",
    "\n",
    "#Plot gradient\n",
    "X, Y = np.meshgrid(np.arange(-3, 3, 0.4), np.arange(-3, 3, 0.4))\n",
    "Z = np.zeros(X.shape)\n",
    "\n",
    "U = np.zeros(X.shape)\n",
    "V = np.zeros(X.shape)\n",
    "W = np.zeros(X.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        g = gradient([X[i][j],Y[i][j]])\n",
    "        U[i][j] = g[0]\n",
    "        V[i][j] = g[1]\n",
    "        \n",
    "ax.quiver(X, Y, Z, U, V, W, length=0.1, color = 'black')\n",
    "\n",
    "#Plot gradient alone\n",
    "plt.figure(2)\n",
    "plt.title('Gradient of function')\n",
    "Q = plt.quiver(X, Y, U, V, units='width')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how the gradient works it is understandable that if we would like to\n",
    "<br>\n",
    "find the minimum of a function we would have to take steps in the opposite\n",
    "<br>\n",
    "direction the gradients points.\n",
    "\n",
    "During the training of a neural networks we would like to modify the weights\n",
    "<br>\n",
    "of the networks in order to minimize the error function.\n",
    "\n",
    "So the basic idea of the gradient descent algorithm is the following:\n",
    " - Initialize the parameters to random values (select a random point in the parameter space)\n",
    " - Compute the gradient of the function at the specified point\n",
    " - Get a new point in the parameter space by taking a step from the current point in the\n",
    " <br>\n",
    " oppsite direction as the gradient.\n",
    " - Repeat this process as long as a condition is met (number of iterations or the error is below\n",
    " <br> a previously defined level.\n",
    " \n",
    "The size of the step we take determines the speed of the conversion. This is called the learning rate\n",
    "<br>\n",
    "and it is a hyperparameter. This means that it is not directly modified during the training process,\n",
    "<br>\n",
    "but it has to be tuned using validation.\n",
    "\n",
    "So the training is an iterative process where the computations of the weights can be formulated like:\n",
    "\n",
    "$$\\mathbf{w}^{i+1} = \\mathbf{w}^{i} - \\eta \\nabla E(\\mathbf{w})(\\mathbf{w}^{i}) \\text{ , so}$$\n",
    "\n",
    "$$w_{j}^{i+1} =w_j^{i} - \\eta \\frac{\\partial E(\\mathbf{w})}{\\partial w_{j}}(\\mathbf{w}^i)$$\n",
    "\n",
    ",where $\\mathbf{w} = [w_1, w_2,\\dots, w_n]$ and  $\\mathbf{w}^{i}$ is a collection of the weights in the $i^{th}$ iteration of the training process\n",
    "<br>\n",
    "in case of  $i = 0$ the weights are initialized with random values.\n",
    "<br>\n",
    "$E(\\mathbf{w})$ is the error function and $ \\nabla E(\\mathbf{w})(\\mathbf{w}^{i})$ is the evaluation of the gradient of the error function in the point $\\mathbf{w}^{i}$\n",
    "<br>\n",
    "$\\eta$ is the learning rate and\n",
    "<br>\n",
    "$j \\in \\{1, 2, \\dots, n\\}$\n",
    "\n",
    "<h3>Learning rate, iteration, batch and epoch</h3>\n",
    "\n",
    "Increasing the learning rate means that we take larger steps durong the training process.\n",
    "<br>\n",
    "This could increase the speed of the training, hoowever a too large learning rate can cause problems\n",
    "<br>\n",
    "in the stability of the training process. The effects of the learning rate can be seen on the figures below:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://srdas.github.io/DLBook/DL_images/TNN2.png\" width=\"60%\"/>\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1600/0*uIa_Dz3czXO5iWyI.\" width=\"40%\"/>\n",
    "</center>\n",
    "\n",
    "The error function is often referred to as the loss function as well.\n",
    "\n",
    "The meaning of an iteration of the training process is a sigle step of weight update.\n",
    "<br>\n",
    "A batch is a collection of training data for which the error is computed before the weight update\n",
    "<br>\n",
    "is performed. In case of Batch Gradient Descent, the whole training dataset is used as a batch.\n",
    "<br>\n",
    "In this case the weight update is performed only after the error is computed for all predicitions\n",
    "<br>\n",
    "on each training samples.\n",
    "<br>\n",
    "In case of the Stochastic Gradient Descent algorithm, weight update is carried out for each training sample.\n",
    "<br>\n",
    "In case of Mini Batch Gradient Descent the batch size is greater than one but smaller than the size of the\n",
    "<br>\n",
    "training dataset.\n",
    "\n",
    "The advantage of the Batch Gradient Descent method is its stability. However, it may get stuck in a suboptimal\n",
    "<br>\n",
    "solution and it requires the whole training set to be loaded in memory.\n",
    "\n",
    "Stochastic Gradient Descent can converge faster than Batch Gradient Descent but it requires more computations\n",
    "<br>\n",
    "due to the frequent weight updates and can result noisy gradients, so the error rate will not decrease gradually.\n",
    "\n",
    "The Mini Batch Gradient Descent can combine the advantages of the Batch and Stochastic Gradient Descent algorithms.\n",
    "<br>\n",
    "That is why choosing the right batch size for the given problem is very important. Just like the learning rate\n",
    "<br>\n",
    "the batch size is a hyperparameter as well.\n",
    "\n",
    "An epoch is a full pass of all the training samples during the training process.\n",
    "\n",
    "Now that we know how to compute the gradient of a multi-variable function and how to perform the training process\n",
    "<br>\n",
    "it is time to inspect the gradient in a neural network.\n",
    "\n",
    "<h3>Derivatives of activation functions in neurons</h3>\n",
    "\n",
    "The activation function of the neurons in the neural network has a great impact on the gradient computation,\n",
    "<br>\n",
    "so the derivatives of the activation functions should be calculated and inspected.\n",
    "\n",
    "| Name | Equation of function, $f(x)$ | Equation of derivative, $f'(x)$ |\n",
    "| ----- | ----- | ----- |\n",
    "| Linear | $f(x) = x$ | f'(x) = 1 |\n",
    "| Step | $f(x)=\\begin{cases} 1 & \\text{if $x \\geq$ 0}\\\\ 0 & \\text{if $x < 0$}\\end{cases}$ | $f'(x)=\\begin{cases} 0 & \\text{if $x \\neq 0$ }\\\\ ? & \\text{if $x = 0$ }\\end{cases}$ |\n",
    "| Sigmoid | $\\sigma (x)=\\frac{1}{1+e^{-x}}$ | $\\sigma '(x) = \\sigma (x)(1-\\sigma (x))$ |\n",
    "| Tangent Hyperbolic | $f(x) = \\tanh(x)$ | $f'(x) = \\frac{1}{x^2+1}$ |\n",
    "| ReLU | $R(x) = \\max{(0,x)}$ | $R'(x)=\\begin{cases} 0 & \\text{if $x \\leq 0$ }\\\\ 1 & \\text{if $x > 0$ }\\end{cases}$ |\n",
    "| Leaky ReLu | $LR(x) = \\max{(\\alpha x,x)}$ | $LR'(x)=\\begin{cases} \\alpha & \\text{if $x \\leq 0$ }\\\\ 1 & \\text{if $x > 0$ }\\end{cases}$ |\n",
    "\n",
    "\n",
    "From this table it is clear why the step function is not used often in neural networks. The slope of the function is\n",
    "<br>\n",
    "zero in most locations, so the gradient cannot be used to determine in which direction should we modify the parameters.\n",
    "\n",
    "The gradients in neural networks can be computed by using the chain rule for derivation, because the calculation of the\n",
    "<br>\n",
    "output can be formulated as a series of multi-variable functions.\n",
    "\n",
    "The final component that should be discussed about the training process is the formulation of the error, or the so-called loss function.\n",
    "\n",
    "<h2>Loss function</h2>\n",
    "\n",
    "The two most popular loss functions for neural networks are the Mean Squared Error and the Cross Entropy loss.\n",
    "<br>\n",
    "The Mean Squared Error is used in case of regression problems, when the error is the difference between continuous values.\n",
    "<br>\n",
    "The Cross Entropy loss is used in case of classification problems, when the output of the network is a probability distibution\n",
    "<br>\n",
    "for the output classes.\n",
    "\n",
    "These loss functions can be formulated like:\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum_{i = 1}^N(z_i - y_i)^2$$\n",
    "\n",
    ",where $MSE$ is the Mean Squared Error for $N$ number of predictions (this will be the batch size)\n",
    "<br>\n",
    "$z_i$ is the predicted value for the $i^{th}$ sample\n",
    "<br>\n",
    "$y_i$ is the correct value for the output if the input is the $i^{th}$ sample\n",
    "\n",
    "and the Cross Entropy is:\n",
    "\n",
    "$$CE = -\\sum_{j = 1}^My_j\\log(p_j)$$\n",
    "\n",
    ",where $CE$ is the Cross Entropy loss for a single prediction on a single training sample\n",
    "<br>\n",
    "$M$ is the number of classes\n",
    "<br>\n",
    "$p_j$ is the $j^{th}$ element of the predicted probability distibution\n",
    "$z_j$ is the $j^{th}$ element of the correct probability distibution\n",
    "\n",
    "For a batch of samples, the sum or the mean of the Cross Entropy loss over the samples can be used.\n",
    "\n",
    "It can be seen that for both loss functions, the correct labels have to be provided. We call these methods\n",
    "<br>\n",
    "supervised learning, because the training samples have to be provided with correct labels.\n",
    "\n",
    "Unsupervised learning can be used to detect similarities in the data and to perform clustering.\n",
    "<br>\n",
    "That is why the loss functions for unsupervised learning methods are usually distance measures.\n",
    "\n",
    "Now let's see a simple exmple on a supervised regression problem using the MSE as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define activation functions:\n",
    "def sigmoid(x):\n",
    "    return (1/ (1 + np.exp(-x)))\n",
    "\n",
    "def linear(x):\n",
    "    return (x)\n",
    "\n",
    "\n",
    "#Function to regress:\n",
    "def function(inp):\n",
    "    return (0.1*inp[0]+0.4*inp[1])\n",
    "\n",
    "\n",
    "#Layers of the network:\n",
    "def layer(inputs, weights, activation):\n",
    "    bias_inputs = np.ones((inputs.shape[0],1))\n",
    "    inputs = np.append(inputs, bias_inputs, axis=1)\n",
    "    return (activation(inputs.dot(weights)))\n",
    "\n",
    "\n",
    "#Prepare inputs:\n",
    "x = np.arange(-3.0, 3.0, 0.05)\n",
    "y = np.arange(-3.0, 3.0, 0.05)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zs = np.array([function([x,y]) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "train_data = np.array([([x,y],function([x,y])) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z = zs.reshape(X.shape)\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "for t in train_data:\n",
    "    inputs.append(t[0])\n",
    "    labels.append(t[1])\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels = np.reshape(labels,[-1,1])\n",
    "\n",
    "\n",
    "#Hyperparameters:\n",
    "number_of_inputs = 2\n",
    "number_of_neurons = [2,1]\n",
    "activations = [sigmoid,linear]\n",
    "learning_rate = 0.1\n",
    "number_of_epochs = 200\n",
    "\n",
    "\n",
    "#Define weights:\n",
    "weights = []\n",
    "for i in range(len(number_of_neurons)):\n",
    "    if i==0:\n",
    "        weights.append(np.random.uniform(-1.0,1.0,(number_of_inputs+1,number_of_neurons[i])))\n",
    "    else:\n",
    "        weights.append(np.random.uniform(-1.0,1.0,(number_of_neurons[i-1]+1,number_of_neurons[i])))\n",
    "\n",
    "\n",
    "#Build network:\n",
    "losses = []\n",
    "\n",
    "def net(inputs, weights, activations):\n",
    "    acts = []\n",
    "    acts.append(inputs)\n",
    "    for i in range(len(weights)):\n",
    "        hidden = layer(acts[-1],weights[i],activations[i])\n",
    "        acts.append(hidden)\n",
    "    return (acts)\n",
    "\n",
    "#Training:\n",
    "for k in range(number_of_epochs):\n",
    "    acts = net(np.array(inputs), weights, activations)\n",
    "    output = acts[-1]\n",
    "\n",
    "    loss = ((output - labels)**2).sum()/(2*len(labels))\n",
    "\n",
    "    print('Epoch', k, 'Loss: ', loss)\n",
    "    losses.append(loss)\n",
    "\n",
    "    dldz = (output-labels) #z-y\n",
    "    dzdw2 = np.append(acts[-2], np.ones((len(acts[-2]),1)), axis=1) #h\n",
    "\n",
    "    dldw2 = np.reshape((dldz*dzdw2).sum(axis=0)/len(labels),[-1,1])\n",
    "\n",
    "    dzdh = np.reshape(weights[1][0:-2],-1) #w_2^T\n",
    "    dhdn = acts[-2]*(1-acts[-2]) #h.(1-h)\n",
    "    dndw1 = np.append(acts[-3], np.ones((len(acts[-3]),1)), axis=1) #x\n",
    "\n",
    "    dldw1 = np.zeros((number_of_neurons[0]+1,number_of_inputs))\n",
    "    for i in range(len(labels)):\n",
    "        dldw1 += np.reshape(dndw1[i],[-1,1]).dot(np.reshape(dldz[i]*dzdh*dhdn[0],[1,-1]))\n",
    "\n",
    "    dldw1 /= len(labels)\n",
    "\n",
    "    weights[0] -= learning_rate*dldw1\n",
    "    weights[1] -= learning_rate*dldw2\n",
    "\n",
    "\n",
    "#Visualize the training and results:\n",
    "plt.figure(1)\n",
    "plt.plot(range(len(losses)),losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Training process')\n",
    "\n",
    "fig = plt.figure(2)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, Z)\n",
    "\n",
    "pred_zs = np.array([net(np.array([[x,y]]), weights, activations)[-1] for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "pred_Z = pred_zs.reshape(X.shape)\n",
    "\n",
    "ax.plot_surface(X, Y, pred_Z)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Function and predictions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above it can be seen that even a very simple neural network is really hard to make\n",
    "<br>\n",
    "if we are coding everything from scratch. That is why in the next block we will learn how to handle\n",
    "<br>\n",
    "the TensorFlow framework. However, first of all, we should discuss the theory of deep learning.\n",
    "\n",
    "Continue: [2.3 Deep Learning Theory](Deep_Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
