{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">3.4 Optimizers and Training Process</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "In this lesson first we will see how to use the built-in optimizers in tensorflow on a simple task.\n",
    "<br>\n",
    "Next we will see, how to create simple neural network structures for classification and regression and how to\n",
    "<br>\n",
    "perform a training process to tune the variables of the models.\n",
    "\n",
    "First of all, import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import SIT_visit.Block_3.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Optimizers</h2>\n",
    "\n",
    "The optimizers provided in TensorFlow can be found in the ```tf.train``` package.\n",
    "<br>\n",
    "The most important ones are the GradientDescentOptimizer, the AdamOptimizer and the MomentumOptimizer.\n",
    "<br>\n",
    "For more info on optizers see https://www.tensorflow.org/api_docs/python/tf/train\n",
    "\n",
    "These optimizers automatically modify the variables of the model, in order to find the minimum of a function.\n",
    "\n",
    "<p style=\"margin-top:2em;\">Now let's see how to use an optimizers to find the minimum value of a simple function $(y=x^4+2x^3-4x^2-4x+5)$, where $x,y \\in \\mathbb{R}$:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that computes y\n",
    "def function(x):\n",
    "    return (x**4+2*x**3-4*x**2-4*x+5)\n",
    "\n",
    "#Plot the function\n",
    "x = np.linspace(-3, 2, 100)\n",
    "\n",
    "plt.plot(x, function(x), color='black')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to store values during all iterations\n",
    "X_values = []\n",
    "Y_values = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Create a variable and initialize it with a random normal distribution\n",
    "x_1 = tf.get_variable('x_1', shape=[], dtype=tf.float32, initializer=tf.initializers.random_normal)\n",
    "\n",
    "#Create a variable and initialize it as zero\n",
    "x_2 = tf.get_variable('x_2', shape=[], dtype=tf.float32, initializer=tf.initializers.zeros)\n",
    "\n",
    "#Create a variable and initialize it with a constant value of -1\n",
    "x_3 = tf.get_variable('x_3', dtype=tf.float32, initializer=-1.0)\n",
    "\n",
    "#Stack all variables into a vector, so the result of the function for each one can be computed in one pass\n",
    "X = tf.stack(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES), axis=0, name='stack')\n",
    "\n",
    "Y = function(X)\n",
    "\n",
    "#Create a gradient descent optimizer and tell it to minimize the value of Y\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(Y)\n",
    "\n",
    "#Create an initializer for the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Run the session as long as the chane in the x values is less than 0.0001\n",
    "train = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    while train:\n",
    "        X_v,Y_v,_ = sess.run([X,Y,train_step])\n",
    "        if not X_values:\n",
    "            X_values.append(X_v)\n",
    "            Y_values.append(Y_v)\n",
    "        else:\n",
    "            if all(abs(X_values[-1]-X_v) < 0.0001):\n",
    "                train = False\n",
    "                X_values.append(X_v)\n",
    "                Y_values.append(Y_v)\n",
    "            else:\n",
    "                X_values.append(X_v)\n",
    "                Y_values.append(Y_v)\n",
    "                \n",
    "print('Training finished')\n",
    "print('The minimum places and values (x,y) found from the initial positions:')\n",
    "for i in range(len(X_values[-1])):\n",
    "    print('x:', X_values[-1][i], 'y:', Y_values[-1][i])\n",
    "\n",
    "#Plot the function and the iteration steps\n",
    "x = np.linspace(-3, 2, 100)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, function(x), label='function', color='black')\n",
    "\n",
    "cm1 = cm.Blues(np.linspace(0.4,1,len(X_values)))\n",
    "cm2 = cm.Reds(np.linspace(0.4,1,len(X_values)))\n",
    "cm3 = cm.YlGn(np.linspace(0.4,1,len(X_values)))\n",
    "\n",
    "ax.scatter([x[0] for x in X_values], [y[0] for y in Y_values], s=100, label='st norm init', color=cm1, marker='^')\n",
    "ax.scatter([x[1] for x in X_values], [y[1] for y in Y_values],  s=100, label='0 init', color=cm2, marker='x')\n",
    "ax.scatter([x[2] for x in X_values], [y[2] for y in Y_values],  s=100, label='-1 init', color=cm3, marker='+')\n",
    "plt.legend()\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Finding minimum from different start points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">From the example above it can be seen, that the Gradient Descent optimization method can\n",
    "<br>\n",
    "be stuck in local minima. That's why the initialization of the variables is very important.</p>\n",
    "\n",
    "Next let's see what difference the optimization process makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to store values during all iterations\n",
    "X_values = []\n",
    "Y_values = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Create variables and initialize them with values of -1\n",
    "x_1 = tf.get_variable('x_1', dtype=tf.float32, initializer=-1.0)\n",
    "\n",
    "x_2 = tf.get_variable('x_2', dtype=tf.float32, initializer=-1.0)\n",
    "\n",
    "x_3 = tf.get_variable('x_3', dtype=tf.float32, initializer=-1.0)\n",
    "\n",
    "#Stack all variables into a vector, so the result of the function for each one can be computed in one pass\n",
    "X = tf.stack(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES), axis=0, name='stack')\n",
    "\n",
    "Y = function(X)\n",
    "\n",
    "#Create optimizers and tell them to minimize the value of Y\n",
    "train_step_1 = tf.train.GradientDescentOptimizer(0.01).minimize(Y, var_list=[x_1])\n",
    "train_step_2 = tf.train.AdamOptimizer(0.1).minimize(Y, var_list=[x_2])\n",
    "train_step_3 = tf.train.MomentumOptimizer(0.01, 0.8).minimize(Y, var_list=[x_3])\n",
    "\n",
    "#Create an initializer for the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Run the session as long as the chane in the x values is less than 0.0001\n",
    "train = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    while train:\n",
    "        X_v,Y_v,_,_,_ = sess.run([X,Y,train_step_1,train_step_2,train_step_3])\n",
    "        if not X_values:\n",
    "            X_values.append(X_v)\n",
    "            Y_values.append(Y_v)\n",
    "        else:\n",
    "            if all(abs(X_values[-1]-X_v) < 0.0001):\n",
    "                train = False\n",
    "                X_values.append(X_v)\n",
    "                Y_values.append(Y_v)\n",
    "            else:\n",
    "                X_values.append(X_v)\n",
    "                Y_values.append(Y_v)\n",
    "                \n",
    "print('Training finished')\n",
    "print('The minimum places and values (x,y) found from the initial positions:')\n",
    "for i in range(len(X_values[-1])):\n",
    "    print('x:', X_values[-1][i], 'y:', Y_values[-1][i])\n",
    "\n",
    "#Plot the function and the iteration steps\n",
    "x = np.linspace(-3, 2, 100)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, function(x), label='function', color='black')\n",
    "\n",
    "cm1 = cm.Blues(np.linspace(0.4,1,len(X_values)))\n",
    "cm2 = cm.Reds(np.linspace(0.4,1,len(X_values)))\n",
    "cm3 = cm.YlGn(np.linspace(0.4,1,len(X_values)))\n",
    "\n",
    "ax.scatter([x[0] for x in X_values], [y[0] for y in Y_values], s=100, label='Gradient Descent', color=cm1, marker='^')\n",
    "ax.scatter([x[1] for x in X_values], [y[1] for y in Y_values],  s=100, label='Adam', color=cm2, marker='x')\n",
    "ax.scatter([x[2] for x in X_values], [y[2] for y in Y_values],  s=100, label='Momentum', color=cm3, marker='+')\n",
    "plt.legend()\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Finding minimum with different optimizers')\n",
    "\n",
    "#Uncomment these to show the behaviour near the minimum point\n",
    "#plt.xlim(-2.3,-2)\n",
    "#plt.ylim(-3.5,-3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">As seen from the previous example, the Adam and the Momentum optimizers settle in the\n",
    "<br>\n",
    "minimum with oscillations. This behaviour is due to the introduced momentum term in their formulation.</p>\n",
    "<br>\n",
    "The Momentum optimizer uses a fix momentum for the weight updates, while the Adam optimizer uses an adaptively decaying momentum.\n",
    "<br>\n",
    "The momentum term can help these optimizers to swing over smaller local minima.\n",
    "\n",
    "<h2>Training Process</h2>\n",
    "\n",
    "Now, let's see two simple examples for the construction of the whole data pipeline from\n",
    "<br>\n",
    "data preparation to the end of the training process. First, we create a fully connected neural network\n",
    "<br>\n",
    "and train it to adapt the classic XOr method.\n",
    "\n",
    "**Problem statement:**\n",
    "\n",
    "There are two logical variables $a$ and $b$ that will be the inputs for our network.\n",
    "<br>\n",
    "We would like to predict the XOr connection between $a$ and $b$ with our model.\n",
    "\n",
    "The truth table of the XOr connection betwwen $a$ and $b$ is:\n",
    "\n",
    "| a | b | XOr |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "We can compute the right labels for the network according to this table.\n",
    "<br>\n",
    "This task can be solved by classification, because the output is a discrete value.\n",
    "<br>\n",
    "It can either be True (1) or False (0).\n",
    "\n",
    "The network should have a hidden layer of two neurons and a single output neuron.\n",
    "<br>\n",
    "The network should look like this:\n",
    "\n",
    "<center>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1vuDFz0vK6tKGyk-uHg4peybJCGcS_L4u\" width=\"30%\"/>\n",
    "</center>\n",
    "\n",
    "The neurons should implement a sigmoid activation function to force the outputs to be\n",
    "<br>\n",
    "between zero and one. We will use the built-in sigmoid activation function in TensorFlow.\n",
    "\n",
    "The output of the network should be compared to the labels and a loss term should be introduced that we can minimize.\n",
    "<br>\n",
    "In this task we will use the binary cross entropy loss function:\n",
    "\n",
    "$$L_{CE}=z(-\\log{p})+(1-z)(-\\log{(1-p)})$$\n",
    "\n",
    "where $z$ is the correct label (0 or 1) and $p$ is the prediction of the network that can be interpreted as\n",
    "<br>\n",
    "the probability of the output being True.\n",
    "\n",
    "In TensorFlow, this loss function is already defined, so we just have to use it.\n",
    "<br>\n",
    "It is also important to note that the loss function in Tensorflow  awaits the output of the network\n",
    "<br>\n",
    "without the sigmoid activation function, as it is applied inside the calculation of the loss for performance purposes.\n",
    "\n",
    "We will use the Gradient Descent optimizer to minimize this loss by modifying the parameters of our network.\n",
    "\n",
    "We have to address the following tasks:\n",
    " - Prepare the training data for the training process\n",
    " - Build the data pipeline for Tensorflow\n",
    " - Construct the neural network model\n",
    " - Define a loss function and select an optimizer for the task\n",
    " - Code the training process\n",
    " - Train the model\n",
    " - Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Data preparation:\n",
    "inputs = list(itertools.product([0, 1], repeat=2))\n",
    "labels = [[a^b] for a,b in inputs]\n",
    "print(inputs)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "#Data pipeline:\n",
    "batch_size = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((inputs,labels)).shuffle(len(labels)).repeat().batch(batch_size)\n",
    "\n",
    "iterator = ds.make_initializable_iterator()\n",
    "iterator_init = iterator.initializer\n",
    "\n",
    "inps,labs = iterator.get_next()\n",
    "inps = tf.identity(inps, name='inputs')\n",
    "labs = tf.identity(labs, name='labels')\n",
    "\n",
    "\n",
    "#Neural network model:\n",
    "def net(x, layer_sizes=[2,1], reuse=False, name='FC'):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        hidden = tf.layers.dense(x, layer_sizes[0], activation=tf.nn.sigmoid, name='hidden_layer')\n",
    "        output = tf.layers.dense(hidden, layer_sizes[1], name='output_layer')\n",
    "        return output\n",
    "\n",
    "logits = net(tf.to_float(inps))\n",
    "predictions = tf.nn.sigmoid(logits, name='predictions')\n",
    "th_predictions = tf.to_int32(predictions > 0.5, name='th_predictions')\n",
    "accuracy = tf.identity(tf.reduce_sum(tf.to_int32(tf.equal(th_predictions,labs)))/batch_size, name='accuracy')\n",
    "\n",
    "\n",
    "#Loss function and optimization:\n",
    "loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.to_float(labs), logits=logits, name='loss'))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "variable_init = tf.global_variables_initializer()\n",
    "\n",
    "#Training process and evaluation:\n",
    "train = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([iterator_init,variable_init])\n",
    "    c = 0\n",
    "    while train:\n",
    "        l,a,_ = sess.run([loss, accuracy, train_step])\n",
    "        print('Batch:', c, 'loss:', l, 'accuracy:', a)\n",
    "        if a==1 or c > 500:\n",
    "            train = False\n",
    "        c += 1\n",
    "    print('\\nEnd of training process\\n')\n",
    "    p,th_p = sess.run([predictions,th_predictions], feed_dict={inps: inputs})\n",
    "    for j in range(len(inputs)):\n",
    "        print('\\nInputs:', inputs[j], 'prediction:', p[j], '\\nth_prediction:', th_p[j], 'correct label:', labels[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Excersise 3.4</h2>\n",
    "\n",
    "Train a fully connected neural network for a regression task!\n",
    "<br>\n",
    "The training data can be found in the Numpy file data.npy in the subdirectory called data.\n",
    "<br>\n",
    "Load the data like:\n",
    "```python\n",
    "inputs,labels = np.load('SIT_visit/Block_3/data/data.npy')\n",
    "```\n",
    "The inputs for the network are single scalars $x$ and the labels $y$ are scalars as well, where $x,y\\in \\mathbb{R},\\quad x\\in[0,1]$\n",
    "\n",
    "You will have to perform the following tasks:\n",
    " - Plot the training data\n",
    " - Prepare the training data for the training process\n",
    " - Build the data pipeline for Tensorflow\n",
    " - Construct the neural network model (Fully connected neural network with two hidden layers and one output neuron)\n",
    " <br>\n",
    " The hidden neurons should have hyperbolic tangent activation and the output neuron does not need an activation function.\n",
    " - You should choose the proper number of neurons in the hidden layers for yoursef.\n",
    " - Use the ```tf.losses.mean_squared_error()``` loss function and the Gradient Descent optimizer and choose a learning rate\n",
    " - Code the training process\n",
    " - Train the model and plot the loss during the training\n",
    " - Evaluate the model (Plot the training data and the predictions for values in the given ```test_data``` array)\n",
    " \n",
    " Extra tasks:\n",
    " - Experiment with the batch size, number of neurons, activation functions, optimizer and learning rate\n",
    " <br>\n",
    " to create models that converge fast and are accurate, or ones that require the least number of parameters.\n",
    " \n",
    " - Build a fully connected network that estimes the parameters $s_i,\\space i \\in \\{0,1,2,3,4,5\\}$ if we suppose\n",
    " <br>\n",
    " that the data can be best approximated with a function like:\n",
    " $$y=s_0\\sin^2(s_1x+s_2)+s_3\\sin(s_4x+s_5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data\n",
    "test_data = np.arange(0,1,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See solution here: [Excersise 3.4 solution](Excersise_3_4.ipynb)\n",
    "\n",
    "Continue: [3.5 TensorFlow Project](TensorFlow_Project.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
