{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">3.3 Variables and Activation Functions</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "In this lesson we will see how to use variables and activation functions in TensorFlow\n",
    "<br>\n",
    "and how to create neural network layers very easily.\n",
    "\n",
    "First of all, import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import SIT_visit.Block_3.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Variables</h2>\n",
    "\n",
    "Variables in TensorFlow are used to preserve values between consecutive evaluations (calls of the run() method of Sessions).\n",
    "<br>\n",
    "There are two ways for creating a variable in TensorFlow. The first one is to call the constructor of the Variable class like this:\n",
    "\n",
    "```python\n",
    "w = tf.Variable(initial_value, name=optional_name)\n",
    "```\n",
    "\n",
    "In this case, the initial value of the variable have to be specified.\n",
    "<br>\n",
    "There are several optional arguments for this method. For detailed info see https://www.tensorflow.org/api_docs/python/tf/Variable\n",
    "\n",
    "The other one is the get_variable method:\n",
    "\n",
    "```python\n",
    "tf.get_variable(\n",
    "    name,\n",
    "    shape=None,\n",
    "    dtype=None,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    trainable=None,\n",
    "    collections=None,\n",
    "    caching_device=None,\n",
    "    partitioner=None,\n",
    "    validate_shape=True,\n",
    "    use_resource=None,\n",
    "    custom_getter=None,\n",
    "    constraint=None,\n",
    "    synchronization=tf.VariableSynchronization.AUTO,\n",
    "    aggregation=tf.VariableAggregation.NONE\n",
    ")\n",
    "```\n",
    "\n",
    "The difference between the two is that calling the constuctor always creates a new variable, while\n",
    "<br>\n",
    "the get_variable method can create a new or return an already existing variable, identified by its name.\n",
    "\n",
    "<h3>Collections</h3>\n",
    "\n",
    "Variables can be grouped in collections for convenient usage. A variable can be added to a collection upon its creation\n",
    "<br>\n",
    "or later, with the use of ```tf.add_to_collection()``` method. By calling ```tf.get_callection()``` the variables can be retirieved.\n",
    "\n",
    "<h3>Initializers</h3>\n",
    "\n",
    "In order to use variables, first they have to be initialized.\n",
    "<br>\n",
    "The variables created with the constructor will be initialized to the given initial value.\n",
    "<br>\n",
    "Variables that were created with the ```tf.get_variable()``` method have an initializer argument, with which the variable\n",
    "<br>\n",
    "can be initialized as a given value, or it can be done by an initializer function.\n",
    "\n",
    "Just in case of iterators, variables in TensorFlow can be initialized by executing their initializer method.\n",
    "<br>\n",
    "There is also a convenient way to initialize all variables. It is the ```tf.global_variables_initializer()``` method.\n",
    "<br>\n",
    "This method initializes all the variables from ```tf.GraphKeys.GLOBAL_VARIABLES``` collection and all variables\n",
    "<br>\n",
    "are added to this collection by default.\n",
    "\n",
    "<p style=\"margin-top:2em;\">Now let's see how variables in TensorFlow can be used:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Reset graph, so the variables won't be created again and again\n",
    "\n",
    "#Create a variable with the constructor\n",
    "w1 = tf.Variable(1, name='var1')\n",
    "\n",
    "#Create a variable with the get_variable() method\n",
    "w2 = tf.get_variable('var2', dtype=tf.int32, initializer=2)\n",
    "\n",
    "#Create a variable with the get_variable() method and initialize it with a built-in tensorflow function\n",
    "#(See tf.initializers for other initialisers)\n",
    "w3 = tf.get_variable('var3', shape=[], dtype=tf.float32, initializer=tf.initializers.random_normal())\n",
    "\n",
    "#Create a variable with the constuctor and name it like w1\n",
    "w4 = tf.Variable(4, name='var1')\n",
    "\n",
    "#Create a variable with the constuctor and add it to a collection named my_collection\n",
    "w5 = tf.Variable(5, name='var5', collections=['my_collection'])\n",
    "\n",
    "#Add w3 to the collcetion named my_collection\n",
    "tf.add_to_collection('my_collection', w3)\n",
    "\n",
    "#Try to get variable var1 with the get_variable() method\n",
    "w6 = tf.get_variable('var1', shape=[])\n",
    "\n",
    "#Get and print the names of all of the created variables\n",
    "names = [v.name for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
    "\n",
    "print('The names of the created variables are:')\n",
    "for name in names:\n",
    "    print(name)\n",
    "\n",
    "#Get and print the names of the variables in the collection named my_collection\n",
    "my_coll_names = [v.name for v in tf.get_collection('my_collection')]\n",
    "\n",
    "print('The variables in the collection named my_collection are:')\n",
    "\n",
    "for name in my_coll_names:\n",
    "    print(name)\n",
    "    \n",
    "#Create initializer for all variables and print their values as well\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "init_w5 = w5.initializer #w5 is only added to the collection named my_collection, so it needs to be initialized separately\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init,init_w5])\n",
    "    w1_v,w2_v,w3_v,w4_v,w5_v,w6_v = sess.run([w1,w2,w3,w4,w5,w6])\n",
    "    print('The value of w1 is: ', w1_v)\n",
    "    print('The value of w2 is: ', w2_v)\n",
    "    print('The value of w3 is: ', w3_v)\n",
    "    print('The value of w4 is: ', w4_v)\n",
    "    print('The value of w5 is: ', w5_v)\n",
    "    print('The value of w6 is: ', w6_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">From the example above two important conclusions can be made:</p>\n",
    "\n",
    " - Even though w4 variable was named like w1, it vas created with the constructor, so a new variable\n",
    " <br>\n",
    " was created with the name ```var1_1```.\n",
    " - This is also true for w6 variable, which also became a new variable named ```var1_2```, so variables created\n",
    " <br>\n",
    " with the constructor cannot be retrieved with the ```tf.get_variable()``` method.\n",
    "\n",
    "The best practice is to **always use** the ```tf.get_variable()``` method to create variables, so it will be easier\n",
    "<br>\n",
    "to share them.\n",
    "\n",
    "Also, it can often happen that we would like to name a lot of variables similarly and we don't want to rely on\n",
    "<br>\n",
    "TensorFlow appending an increasing number at the end of names to be unique. That's why scopes exist.\n",
    "<br>\n",
    "There are two kinds of scopes, variable scope and name scope. The difference is that ```tf.get_variable()``` method\n",
    "<br>\n",
    "ignores name scope, because it is assumed that we know exactly the name and the scope of the variable we would like to use.\n",
    "\n",
    "With the scopes we also can organize blocks of tensors and operation in the graph. This can be visualized with the\n",
    "<br>\n",
    "already introduced utility function, ```show_graph()```.\n",
    "\n",
    "<p style=\"margin-top:2em;\">Let's see how these scopes can be used:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Reset graph, so the variables won't be created again and again\n",
    "\n",
    "#Create variables in the variable scope my_var_scope\n",
    "with tf.variable_scope('my_var_scope'):\n",
    "    w1 = tf.get_variable('var1', initializer=2)\n",
    "    w2 = tf.Variable(1, name='var2')\n",
    "\n",
    "#Create variables in the name scope my_name_scope\n",
    "with tf.name_scope('my_name_scope'):\n",
    "    w3 = tf.Variable(2, name='var3')\n",
    "    w4 = tf.get_variable('var4', shape=[])\n",
    "    \n",
    "#Print the names of all variables\n",
    "names = [v.name for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
    "\n",
    "print('The names for the created variables are:')\n",
    "for name in names:\n",
    "    print(name)\n",
    "    \n",
    "#Get variable w1 in the name scope my_name_scope\n",
    "with tf.name_scope('my_name_scope'):\n",
    "    with tf.variable_scope('my_var_scope', reuse=True):\n",
    "        #Setting reuse to True means we would like to use the already created var1 variable and not to create a new one\n",
    "        w5 = tf.get_variable('var1', dtype=tf.int32)\n",
    "    print('In my_name_scope we can now use variable', w5.name)\n",
    "\n",
    "#Check if a new variable was added or not\n",
    "new_names = [v.name for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
    "\n",
    "print('A new variable was created?', names != new_names)\n",
    "\n",
    "#Get w4 variable\n",
    "with tf.variable_scope('', reuse=True):\n",
    "    w6 = tf.get_variable('var4')\n",
    "    \n",
    "#Check if this is the same variable as w4\n",
    "print('w4 and w6 is the same variable?', w4==w6)\n",
    "\n",
    "#Visualize the created graph\n",
    "utils.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">The use of both types of scopes in the code can be a bit confusing, so the best\n",
    "<br>\n",
    "practice is to use only the variable scope.</p>\n",
    "\n",
    "Now that all the important aspects of variables are covered, let's see how they can be utilized to keep their value\n",
    "<br>\n",
    "between call of the ```run()``` method of the TensorFlow Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "#Create variables in two different variable scopes, sum them all and evaluate the results with incrementing different variables\n",
    "with tf.variable_scope('Block1'):\n",
    "    a = tf.get_variable('a', initializer=1)\n",
    "    b = tf.get_variable('b', initializer=2)\n",
    "    c = a+b\n",
    "\n",
    "with tf.variable_scope('Block2'):\n",
    "    a2 = tf.get_variable('a', initializer=1)\n",
    "    b2 = tf.get_variable('b', initializer=2)\n",
    "    c2 = a2+b2+c\n",
    "\n",
    "\n",
    "#Visualize the graph    \n",
    "utils.show_graph(tf.get_default_graph())\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(3):\n",
    "        _,a_v,b_v,c_v,a2_v,b2_v,c2_v = sess.run([tf.assign_add(a,1),a,b,c,a2,b2,c2]) #The tf.assign_add() method increments a\n",
    "        print(a_v,'+', b_v, '=', c_v, 'and', a2_v, '+', b2_v, '+', c_v, '=', c2_v)\n",
    "    for i in range(3):\n",
    "        _,a_v,b_v,c_v,a2_v,b2_v,c2_v = sess.run([tf.assign_add(a2,1),a,b,c,a2,b2,c2]) #The tf.assign_add() method increments a2\n",
    "        print(a_v,'+', b_v, '=', c_v, 'and', a2_v, '+', b2_v, '+', c_v, '=', c2_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">The modifiction of the variables is usually done through the optimizers in ThesorFlow,\n",
    "<br>\n",
    "so this simple example is only for the demonstration of their basic functioning.</p>\n",
    "\n",
    "<h2>Activation Functions</h2>\n",
    "\n",
    "TensorFlow provides several activation functions out of the box, so we will not have to implement them.\n",
    "<br>\n",
    "The most important ones:\n",
    "\n",
    " - ```tf.nn.sigmoid()```\n",
    " - ```tf.nn.softmax()```\n",
    " - ```tf.nn.tanh()```\n",
    " - ```tf.nn.relu()```\n",
    " \n",
    "For other activation functions wisit https://www.tensorflow.org/api_docs/python/tf/nn\n",
    "\n",
    "<p style=\"margin-top:2em;\">A simple fully connected layer with sigmoid activation would look like this:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def fully_connected(inputs, neuron_number, name):\n",
    "    with tf.variable_scope(name, initializer=tf.initializers.random_uniform()):\n",
    "        weights = tf.get_variable('weights', shape=[inputs.shape[1],neuron_number])\n",
    "        biases = tf.get_variable('biases', shape=[neuron_number], initializer=tf.initializers.zeros())\n",
    "        return (tf.nn.sigmoid(tf.matmul(inputs, weights) + biases))\n",
    "    \n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[None, 3])\n",
    "\n",
    "result = fully_connected(inputs, 3, 'fc_1')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Visualize the graph\n",
    "utils.show_graph(tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    res = sess.run(result, feed_dict={inputs: [[0.1,0.1,0.1],[0.1,0.2,0.3],[0.3,0.3,0.3],[0.3,0.3,0.7]]})\n",
    "    print('The results of the inferences are:\\n', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-top:2em;\">Luckily the basic layer types are already defined in TensorFlow, so we will not need to\n",
    "<br>\n",
    "implement them like in the example above.</p>\n",
    "\n",
    "The ```tf.layers``` package has a collection of the most important types of layers in deep learning models.\n",
    "<br>\n",
    "With the use of the functions in this module, the above code can be further simplified.\n",
    "\n",
    "The full list of available methods and classes can be found at https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "\n",
    "<p style=\"margin-top:2em;\">Let's see how to create a similar fully connected layer like the example above with the help of the built-in TensorFlow methods:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[None, 3])\n",
    "\n",
    "result = tf.layers.dense(inputs, 3, activation=tf.nn.sigmoid, kernel_initializer=tf.initializers.random_uniform, name='fc_1')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Visualize the graph\n",
    "utils.show_graph(tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    res = sess.run(result, feed_dict={inputs: [[0.1,0.1,0.1],[0.1,0.2,0.3],[0.3,0.3,0.3],[0.3,0.3,0.7]]})\n",
    "    print('The results of the inferences are:\\n', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Excersise 3.3</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See solution here: [Excersise 3.3 solution](Excersise_3_3.ipynb)\n",
    "\n",
    "Continue: [3.4 Optimizers and Training Process](Optimizers_Training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
